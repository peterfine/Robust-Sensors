<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<title>Robust Sensors: C:/Users/Peter/Documents/Minimax/New folder/EchoStateRobustSensors - SAB submission/EchoStateRobustSensors/src/Networks/OldTanCTRNNetwork.cpp Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/search.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript">
$(document).ready(initResizable);
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css"/>
</head>
<body onload='searchBox.OnSelectItem(0);'>
<!-- Generated by Doxygen 1.7.4 -->
<script type="text/javascript"><!--
var searchBox = new SearchBox("searchBox", "search",false,'Search');
--></script>
<div id="top">
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Robust Sensors</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li class="current"><a href="files.html"><span>Files</span></a></li>
      <li id="searchli">
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="files.html"><span>File&#160;List</span></a></li>
      <li><a href="globals.html"><span>File&#160;Members</span></a></li>
    </ul>
  </div>
</div>
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
  initNavTree('_old_tan_c_t_r_n_network_8cpp.html','');
</script>
<div id="doc-content">
<div class="header">
  <div class="headertitle">
<div class="title">C:/Users/Peter/Documents/Minimax/New folder/EchoStateRobustSensors - SAB submission/EchoStateRobustSensors/src/Networks/OldTanCTRNNetwork.cpp</div>  </div>
</div>
<div class="contents">
<a href="_old_tan_c_t_r_n_network_8cpp.html">Go to the documentation of this file.</a><div class="fragment"><pre class="fragment"><a name="l00001"></a>00001 <span class="comment">/*******************************************************************************</span>
<a name="l00002"></a>00002 <span class="comment"> * TanCTRNNetwork.cpp</span>
<a name="l00003"></a>00003 <span class="comment"> * </span>
<a name="l00004"></a>00004 <span class="comment"> * Peter Fine - EASy MSC, Alife Coursework</span>
<a name="l00005"></a>00005 <span class="comment"> * Created November &#39;04</span>
<a name="l00006"></a>00006 <span class="comment"> ******************************************************************************/</span>
<a name="l00007"></a>00007 
<a name="l00008"></a>00008 <span class="keyword">using namespace </span>std;
<a name="l00009"></a>00009 
<a name="l00010"></a>00010 <span class="preprocessor">#include &quot;<a class="code" href="_tan_c_t_r_n_network_8h.html">TanCTRNNetwork.h</a>&quot;</span>
<a name="l00011"></a>00011 <span class="preprocessor">#include &quot;<a class="code" href="_matlab_writer_8h.html">MatlabWriter.h</a>&quot;</span>
<a name="l00012"></a>00012 <span class="preprocessor">#include &quot;<a class="code" href="_properties_8h.html">Properties.h</a>&quot;</span>
<a name="l00013"></a>00013 <span class="preprocessor">#include &quot;<a class="code" href="_genotype_8h.html">Genotype.h</a>&quot;</span>
<a name="l00014"></a>00014 <span class="preprocessor">#include &quot;tnt126/tnt.h&quot;</span>
<a name="l00015"></a>00015 <span class="preprocessor">#include &quot;jama125/jama_eig.h&quot;</span>
<a name="l00016"></a>00016 
<a name="l00017"></a>00017 <span class="comment">// TEMP</span>
<a name="l00018"></a>00018 <span class="preprocessor">#include &lt;iostream&gt;</span>
<a name="l00019"></a>00019 <span class="preprocessor">#include &lt;sstream&gt;</span>
<a name="l00020"></a>00020 
<a name="l00021"></a><a class="code" href="class_tan_c_t_r_n_network.html#aa6f9f687dc3cd56ebd2ae4c7e719cb0c">00021</a> <a class="code" href="class_tan_c_t_r_n_network.html#aa6f9f687dc3cd56ebd2ae4c7e719cb0c">TanCTRNNetwork::TanCTRNNetwork</a>(<a class="code" href="class_properties.html">Properties</a>&amp; properties) :
<a name="l00022"></a>00022                         <a class="code" href="class_neural_network.html">NeuralNetwork</a>(properties.getInt(<span class="stringliteral">&quot;noInputs&quot;</span>), properties.getInt(<span class="stringliteral">&quot;noOutputs&quot;</span>)),
<a name="l00023"></a>00023                         myNoNeurons(myNoInputs + properties.getInt(<span class="stringliteral">&quot;noInterneurons&quot;</span>) + myNoOutputs),
<a name="l00024"></a>00024                         myNewStateBuffer(myNoNeurons),
<a name="l00025"></a>00025                         myStates(myNoNeurons),
<a name="l00026"></a>00026                         myPostTanhStateBuffer(myNoNeurons),
<a name="l00027"></a>00027                         myWeights(myNoNeurons * myNoNeurons),
<a name="l00028"></a>00028                         myBiases(myNoNeurons),
<a name="l00029"></a>00029                         myTimeConstants(myNoNeurons),
<a name="l00030"></a>00030                         myOutputWeights(myNoOutputs),
<a name="l00031"></a>00031                         myOutputBiases(myNoOutputs),
<a name="l00032"></a>00032                         myTimeStep(properties.getDouble(<span class="stringliteral">&quot;timestep&quot;</span>)),
<a name="l00033"></a>00033                         mySeedWithCentreCrossing(properties.getBool(<span class="stringliteral">&quot;seedWithCentreCrossing&quot;</span>)),
<a name="l00034"></a>00034                         mySeedWithScaledWeights(properties.getBool(<span class="stringliteral">&quot;seedWithScaledWeights&quot;</span>)),
<a name="l00035"></a>00035                         myConstrainWithScaledWeights(properties.getBool(<span class="stringliteral">&quot;constrainWithScaledWeights&quot;</span>)),
<a name="l00036"></a>00036                         myUseLookup(properties.getBool(<span class="stringliteral">&quot;useLookup&quot;</span>)),
<a name="l00037"></a>00037                         myScaleOutputToPositive(properties.getBool(<span class="stringliteral">&quot;scaleOutputToPositive&quot;</span>)),
<a name="l00038"></a>00038                         mySetTausToOne(properties.getBool(<span class="stringliteral">&quot;setTanhTausToOne&quot;</span>)),
<a name="l00039"></a>00039                         mySetBiasesToZero(properties.getBool(<span class="stringliteral">&quot;setBiasesToZero&quot;</span>)),
<a name="l00040"></a>00040                         myUseOutputWeightsAndBiases(properties.getBool(<span class="stringliteral">&quot;useOutputWeightsAndBiases&quot;</span>)),
<a name="l00041"></a>00041                         myScaleModifier(properties.getDouble(<span class="stringliteral">&quot;scaleModifier&quot;</span>)),
<a name="l00042"></a>00042                         myUseStrictScalingCondition(properties.getBool(<span class="stringliteral">&quot;strictScalingCondition&quot;</span>)),
<a name="l00043"></a>00043                         myStrictScalingType(myNoNeurons),
<a name="l00044"></a>00044                         myMaxWeight(properties.getDouble(<span class="stringliteral">&quot;maxWeight&quot;</span>)),
<a name="l00045"></a>00045                         myInitialWeightModifier(properties.getDouble(<span class="stringliteral">&quot;initalWeightModifier&quot;</span>)),
<a name="l00046"></a>00046                         myInitialWeightsNormal(properties.getBool(<span class="stringliteral">&quot;initialWeightsNormal&quot;</span>))
<a name="l00047"></a>00047                         {
<a name="l00048"></a>00048         
<a name="l00049"></a>00049         <span class="comment">// Check that we&#39;re not both seeding and constraining the scaled weights.</span>
<a name="l00050"></a>00050         <span class="keywordflow">if</span>(mySeedWithScaledWeights &amp;&amp; myConstrainWithScaledWeights) {
<a name="l00051"></a>00051                 cout &lt;&lt; <span class="stringliteral">&quot;Both mySeedWithScaledWeights and myConstrainWithScaledWeights were true, quitting!&quot;</span> &lt;&lt; endl;
<a name="l00052"></a>00052                 exit(1);
<a name="l00053"></a>00053         }
<a name="l00054"></a>00054         
<a name="l00055"></a>00055         <span class="comment">// Initialize the tanh lookup table.</span>
<a name="l00056"></a>00056         <span class="keywordflow">for</span> (<span class="keywordtype">double</span> val = -10; val &lt;= 10; val+= 0.002) {
<a name="l00057"></a>00057                 myTanhLookupTable.push_back(tanh(val));
<a name="l00058"></a>00058         }
<a name="l00059"></a>00059 }
<a name="l00060"></a>00060 
<a name="l00061"></a>00061 <span class="keywordtype">double</span> TanCTRNNetwork::calcTanh(<span class="keyword">const</span> <span class="keywordtype">double</span> input) {
<a name="l00062"></a>00062 
<a name="l00063"></a>00063         <span class="keywordflow">if</span> (myUseLookup) {
<a name="l00064"></a>00064                 <span class="comment">/* ROUNDED */</span>
<a name="l00065"></a>00065                 <span class="keywordtype">int</span> index = (int)round((input / 0.002) + 5000.0);
<a name="l00066"></a>00066                 <span class="keywordflow">if</span> (index &lt; 0) {
<a name="l00067"></a>00067                         <span class="keywordflow">return</span> -1.0;
<a name="l00068"></a>00068                 } <span class="keywordflow">else</span> <span class="keywordflow">if</span> (index &gt; 9999) {
<a name="l00069"></a>00069                         <span class="keywordflow">return</span> 1.0;
<a name="l00070"></a>00070                 } <span class="keywordflow">else</span> {
<a name="l00071"></a>00071                         <span class="keywordflow">return</span> myTanhLookupTable[index];
<a name="l00072"></a>00072                 }
<a name="l00073"></a>00073         } <span class="keywordflow">else</span> {
<a name="l00074"></a>00074                 <span class="comment">/* EXACT */</span>
<a name="l00075"></a>00075                 <span class="keywordflow">return</span> tanh(input);
<a name="l00076"></a>00076         }
<a name="l00077"></a>00077 
<a name="l00078"></a>00078         <span class="comment">/* INTERPOLATED - NOT RECODED FOR TANH</span>
<a name="l00079"></a>00079 <span class="comment">         double exactIndex = (input / 0.002) + 5000;</span>
<a name="l00080"></a>00080 <span class="comment">         int index = (int)floor(exactIndex);    </span>
<a name="l00081"></a>00081 <span class="comment">         if(index &lt; 0.0) { return 0.0; }</span>
<a name="l00082"></a>00082 <span class="comment">         else if (index &gt; 9998) { return 1.0; }</span>
<a name="l00083"></a>00083 <span class="comment">         else {</span>
<a name="l00084"></a>00084 <span class="comment">         double resLower = mySigmoidLookupTable.at(index);</span>
<a name="l00085"></a>00085 <span class="comment">         double resUpper = mySigmoidLookupTable.at(index+1);</span>
<a name="l00086"></a>00086 <span class="comment">         return resLower + (exactIndex - index) * (resUpper - resLower);</span>
<a name="l00087"></a>00087 <span class="comment">         }</span>
<a name="l00088"></a>00088 <span class="comment">         */</span>
<a name="l00089"></a>00089 }
<a name="l00090"></a>00090 
<a name="l00091"></a><a class="code" href="class_tan_c_t_r_n_network.html#a5b8248449dde9629c0cc546e55d84b61">00091</a> <span class="keywordtype">void</span> <a class="code" href="class_tan_c_t_r_n_network.html#a5b8248449dde9629c0cc546e55d84b61">TanCTRNNetwork::reconstructNetwork</a>(<a class="code" href="class_genotype.html">Genotype</a>&amp; genotype) {
<a name="l00092"></a>00092 
<a name="l00093"></a>00093         <span class="comment">//  Initialise the parameters according to the genotype.</span>
<a name="l00094"></a>00094         <span class="keywordtype">int</span> gene = 0;
<a name="l00095"></a>00095 
<a name="l00096"></a>00096         <span class="comment">/* Reconstruct the weights between the neurons (ommiting inputs to the sensory</span>
<a name="l00097"></a>00097 <span class="comment">         * neurons). Note these GENE NUMBERS and BOUNDS are expected to be at the begining of the genome by the</span>
<a name="l00098"></a>00098 <span class="comment">         * weight scaling algorithm below.</span>
<a name="l00099"></a>00099 <span class="comment">         */</span>
<a name="l00100"></a>00100         <span class="keywordflow">for</span> (<span class="keywordtype">int</span> dest = 0; dest &lt; myNoNeurons; dest++) {
<a name="l00101"></a>00101                 <span class="keywordflow">for</span> (<span class="keywordtype">int</span> source = 0; source &lt; myNoNeurons; source++) {
<a name="l00102"></a>00102                         <span class="comment">// Note inputs now have incoming weights. Is this good?</span>
<a name="l00103"></a>00103                         <span class="keywordflow">if</span>(myConstrainWithScaledWeights &amp;&amp; (dest == source)) {
<a name="l00104"></a>00104                                 <span class="keywordflow">if</span>(genotype.<a class="code" href="class_genotype.html#ac440f7d0436f1e989869183707b2ff5e">myHasEverBeenConstructed</a> == <span class="keyword">false</span>) {
<a name="l00105"></a>00105                                         genotype.<a class="code" href="class_genotype.html#a86c48c2c8760026f02991b92065d6242">myGenes</a>.at(gene) = 0.0;
<a name="l00106"></a>00106                                 } <span class="comment">//XXX NOTE SELF WEIGHTS NEGATIVE, if changed, change saving of genes after scaling as well!</span>
<a name="l00107"></a>00107                                 myWeights[dest * myNoNeurons + source] = -1 * genotype.<a class="code" href="class_genotype.html#a86c48c2c8760026f02991b92065d6242">myGenes</a>.at(gene) * myMaxWeight;
<a name="l00108"></a>00108                         }
<a name="l00109"></a>00109                                 <span class="keywordflow">else</span> {
<a name="l00110"></a>00110                                         myWeights[dest * myNoNeurons + source] = genotype.<a class="code" href="class_genotype.html#a86c48c2c8760026f02991b92065d6242">myGenes</a>.at(gene) * myMaxWeight * 2 - myMaxWeight;
<a name="l00111"></a>00111                                 }
<a name="l00112"></a>00112                         gene++;
<a name="l00113"></a>00113                 }
<a name="l00114"></a>00114         }
<a name="l00115"></a>00115 
<a name="l00116"></a>00116         <span class="comment">// Sensor weights.</span>
<a name="l00117"></a>00117         mySensorWeight = genotype.<a class="code" href="class_genotype.html#a86c48c2c8760026f02991b92065d6242">myGenes</a>.at(gene) * myMaxWeight * 2 - myMaxWeight;
<a name="l00118"></a>00118         gene++;
<a name="l00119"></a>00119 
<a name="l00120"></a>00120         <span class="comment">// Output weights and biases (if required).</span>
<a name="l00121"></a>00121         <span class="keywordflow">if</span>(myUseOutputWeightsAndBiases) {
<a name="l00122"></a>00122                 <span class="keywordflow">for</span>(<span class="keywordtype">int</span> o = 0; o &lt; <a class="code" href="class_neural_network.html#a18b0f9abda2d0127257162a605c3ac1a">myNoOutputs</a>; o++) {
<a name="l00123"></a>00123                         myOutputWeights[o] = genotype.<a class="code" href="class_genotype.html#a86c48c2c8760026f02991b92065d6242">myGenes</a>.at(gene) * myMaxWeight * 2 - myMaxWeight;
<a name="l00124"></a>00124                         gene++;
<a name="l00125"></a>00125                         myOutputBiases[o] = genotype.<a class="code" href="class_genotype.html#a86c48c2c8760026f02991b92065d6242">myGenes</a>.at(gene) * myMaxWeight * 2 - myMaxWeight;
<a name="l00126"></a>00126                         gene++;
<a name="l00127"></a>00127                 }
<a name="l00128"></a>00128         }
<a name="l00129"></a>00129 
<a name="l00130"></a>00130         <span class="comment">/* If the population should be seeded with centre-crossing networks, and the network has never been constructed</span>
<a name="l00131"></a>00131 <span class="comment">         * before, ALTER THE GENES so the bias genes are centre crossing.</span>
<a name="l00132"></a>00132 <span class="comment">         */</span>
<a name="l00133"></a>00133         <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; myNoNeurons; i++) {
<a name="l00134"></a>00134                 <span class="keywordflow">if</span> (mySeedWithCentreCrossing &amp;&amp; genotype.<a class="code" href="class_genotype.html#ac440f7d0436f1e989869183707b2ff5e">myHasEverBeenConstructed</a> == <span class="keyword">false</span>) {
<a name="l00135"></a>00135                         genotype.<a class="code" href="class_genotype.html#a7470f2d64c23db3b5345d13c541fee0b">setGene</a>(gene, 0.5); <span class="comment">// XXX IS THIS RIGHT FOR CENTRE CROSSING IN TANH???</span>
<a name="l00136"></a>00136                 }
<a name="l00137"></a>00137                 
<a name="l00138"></a>00138                 myBiases[i] = genotype.<a class="code" href="class_genotype.html#a86c48c2c8760026f02991b92065d6242">myGenes</a>.at(gene) * myMaxWeight * 2 - myMaxWeight;
<a name="l00139"></a>00139                 gene++;
<a name="l00140"></a>00140 
<a name="l00141"></a>00141                 <span class="comment">// Set time constants, scaled exponentially to [exp(0) exp(5)]</span>
<a name="l00142"></a>00142                 myTimeConstants[i] = exp(genotype.<a class="code" href="class_genotype.html#a86c48c2c8760026f02991b92065d6242">myGenes</a>.at(gene) * 5);
<a name="l00143"></a>00143                 gene++;
<a name="l00144"></a>00144                 
<a name="l00145"></a>00145                 <span class="comment">// If requested, actually set the time constants to 1.</span>
<a name="l00146"></a>00146                 <span class="keywordflow">if</span>(mySetTausToOne) { myTimeConstants[i] = 1.0; }
<a name="l00147"></a>00147                 
<a name="l00148"></a>00148                 <span class="comment">// If requested, actually set the biases to 0.</span>
<a name="l00149"></a>00149                 <span class="keywordflow">if</span>(mySetBiasesToZero) { myBiases[i] = 0.0; }
<a name="l00150"></a>00150                 
<a name="l00151"></a>00151                 <span class="comment">// If the weights are to be scaled, and the strict condition is being used, then an array of the scaling type for each neuron is evolved.</span>
<a name="l00152"></a>00152                 <span class="keywordflow">if</span>(myConstrainWithScaledWeights &amp;&amp; myUseStrictScalingCondition) {
<a name="l00153"></a>00153                         <span class="keywordflow">if</span>(genotype.<a class="code" href="class_genotype.html#a86c48c2c8760026f02991b92065d6242">myGenes</a>.at(gene) &lt; 0.5) { 
<a name="l00154"></a>00154                                 myStrictScalingType[i] = <span class="keyword">true</span>; 
<a name="l00155"></a>00155                         }
<a name="l00156"></a>00156                                 <span class="keywordflow">else</span> { 
<a name="l00157"></a>00157                                         myStrictScalingType[i] = <span class="keyword">false</span>; 
<a name="l00158"></a>00158                                 }
<a name="l00159"></a>00159                         gene++;
<a name="l00160"></a>00160                 }
<a name="l00161"></a>00161         }
<a name="l00162"></a>00162 
<a name="l00163"></a>00163         <span class="comment">// Scale the weights if required. ---------------------------------------------------------------------------------------</span>
<a name="l00164"></a>00164         <span class="comment">// First, determine whether the weights should be scaled or not.</span>
<a name="l00165"></a>00165         
<a name="l00166"></a>00166         <span class="comment">// Scale the weights once, an write them back into the genome.</span>
<a name="l00167"></a>00167         <span class="keywordflow">if</span> (mySeedWithScaledWeights &amp;&amp; (genotype.<a class="code" href="class_genotype.html#ac440f7d0436f1e989869183707b2ff5e">myHasEverBeenConstructed</a> == <span class="keyword">false</span>)) {          <span class="comment">// Note must have initialized time constants before running this!</span>
<a name="l00168"></a>00168                 scaleWeights(genotype);
<a name="l00169"></a>00169                 
<a name="l00170"></a>00170                 <span class="comment">// Write the weights back into the genome.</span>
<a name="l00171"></a>00171                 <span class="keywordtype">int</span> weightGeneNo = 0;
<a name="l00172"></a>00172                 <span class="keywordflow">for</span> (<span class="keywordtype">int</span> dest = 0; dest &lt; myNoNeurons; dest++) {
<a name="l00173"></a>00173                         <span class="keywordflow">for</span> (<span class="keywordtype">int</span> source = 0; source &lt; myNoNeurons; source++) {
<a name="l00174"></a>00174                                 <span class="comment">// Note inputs now have incoming weights. Is this good?</span>
<a name="l00175"></a>00175                                 genotype.<a class="code" href="class_genotype.html#a86c48c2c8760026f02991b92065d6242">myGenes</a>.at(weightGeneNo) = (myWeights[dest * myNoNeurons + source] + myMaxWeight) / (2 * myMaxWeight);
<a name="l00176"></a>00176                                 <span class="comment">// and clip. // XXX DOES THIS OFTEN HAPPEN???</span>
<a name="l00177"></a>00177                                 <span class="keywordflow">if</span>(genotype.<a class="code" href="class_genotype.html#a86c48c2c8760026f02991b92065d6242">myGenes</a>.at(weightGeneNo) &gt; 1.0) genotype.<a class="code" href="class_genotype.html#a86c48c2c8760026f02991b92065d6242">myGenes</a>.at(weightGeneNo) = 1.0;
<a name="l00178"></a>00178                                 <span class="keywordflow">if</span>(genotype.<a class="code" href="class_genotype.html#a86c48c2c8760026f02991b92065d6242">myGenes</a>.at(weightGeneNo) &lt; 0.0) genotype.<a class="code" href="class_genotype.html#a86c48c2c8760026f02991b92065d6242">myGenes</a>.at(weightGeneNo) = 0.0;
<a name="l00179"></a>00179 
<a name="l00180"></a>00180                                 weightGeneNo++;
<a name="l00181"></a>00181                         }
<a name="l00182"></a>00182                 }
<a name="l00183"></a>00183         }
<a name="l00184"></a>00184         
<a name="l00185"></a>00185         <span class="comment">// Scale the weights every time, but do not write them back into the genome.</span>
<a name="l00186"></a>00186         <span class="keywordflow">if</span>(myConstrainWithScaledWeights) scaleWeights(genotype); <span class="comment">// XXX Does the centrecrossing-ness need to be reaplied for this to work?</span>
<a name="l00187"></a>00187         
<a name="l00188"></a>00188         <span class="comment">// Record in the genotype that it has been reconstructed.</span>
<a name="l00189"></a>00189         genotype.<a class="code" href="class_genotype.html#ac440f7d0436f1e989869183707b2ff5e">myHasEverBeenConstructed</a> = <span class="keyword">true</span>;
<a name="l00190"></a>00190 }
<a name="l00191"></a>00191 
<a name="l00192"></a>00192 <span class="comment">// Only used by the necessary condition, not the sufficient one.</span>
<a name="l00193"></a><a class="code" href="class_tan_c_t_r_n_network.html#a315fc57bca03719f87ea6283b1956720">00193</a> <span class="keywordtype">bool</span> <a class="code" href="class_tan_c_t_r_n_network.html#a315fc57bca03719f87ea6283b1956720">TanCTRNNetwork::checkGenomeIsValid</a>(<a class="code" href="class_genotype.html">Genotype</a>&amp; aGenotype) {
<a name="l00194"></a>00194         
<a name="l00195"></a>00195         <span class="keywordflow">if</span>((myConstrainWithScaledWeights == <span class="keyword">false</span>) || myUseStrictScalingCondition) {
<a name="l00196"></a>00196                 <span class="comment">// Always valid, since we&#39;re either not scaling weights, or using a different, stricter condition.</span>
<a name="l00197"></a>00197                 <span class="keywordflow">return</span> <span class="keyword">true</span>;
<a name="l00198"></a>00198         }
<a name="l00199"></a>00199         
<a name="l00200"></a>00200         <span class="comment">/* Create a copy of the genome, which can be constructed without having to worry</span>
<a name="l00201"></a>00201 <span class="comment">         * about whether it changes the genome at all.</span>
<a name="l00202"></a>00202 <span class="comment">         */</span>
<a name="l00203"></a>00203         <a class="code" href="class_genotype.html">Genotype</a> dummyGenotype = aGenotype;
<a name="l00204"></a>00204         <a class="code" href="class_tan_c_t_r_n_network.html#a5b8248449dde9629c0cc546e55d84b61">reconstructNetwork</a>(dummyGenotype);
<a name="l00205"></a>00205         
<a name="l00206"></a>00206         <span class="comment">// Check whether the weight matrix is stable. First, calculate the full jacobian...</span>
<a name="l00207"></a>00207         
<a name="l00208"></a>00208         <span class="comment">// Put the weights in the format required by the JAMA eigen decomposition class.</span>
<a name="l00209"></a>00209         TNT::Array2D&lt;double&gt; jacobianTNT(myNoNeurons, myNoNeurons);
<a name="l00210"></a>00210         <span class="keywordflow">for</span> (<span class="keywordtype">int</span> source = 0; source &lt; myNoNeurons; source++) {
<a name="l00211"></a>00211                 <span class="keywordflow">for</span> (<span class="keywordtype">int</span> dest = 0; dest &lt; myNoNeurons; dest++) {
<a name="l00212"></a>00212                         <span class="keywordtype">double</span> identityModifier = 0.0;
<a name="l00213"></a>00213                         <span class="keywordflow">if</span>(dest == source) { identityModifier = 1.0; }
<a name="l00214"></a>00214                         
<a name="l00215"></a>00215                                 <span class="comment">// XXX RIGHT WAY ROUND ??? Don&#39;t think it matters!</span>
<a name="l00216"></a>00216                                 jacobianTNT[source][dest] = (myWeights[dest * myNoNeurons + source] - identityModifier) / 
<a name="l00217"></a>00217                                                                                         myTimeConstants[dest];  
<a name="l00218"></a>00218                 }
<a name="l00219"></a>00219         }
<a name="l00220"></a>00220         
<a name="l00221"></a>00221         <span class="comment">// Get the largest real positive eigenvalues.</span>
<a name="l00222"></a>00222         JAMA::Eigenvalue&lt;double&gt; eigen(jacobianTNT);
<a name="l00223"></a>00223         TNT::Array1D&lt;double&gt; eigenValues;
<a name="l00224"></a>00224         eigen.getRealEigenvalues(eigenValues);
<a name="l00225"></a>00225         
<a name="l00226"></a>00226         <span class="keywordtype">double</span> largestEigen = eigenValues[0]; 
<a name="l00227"></a>00227         <span class="keywordflow">for</span>(<span class="keywordtype">int</span> i = 1; i &lt; myNoNeurons; i++) { <span class="comment">// Note starts at 1 due to largestEigen&#39;s initialization above.</span>
<a name="l00228"></a>00228                 <span class="keywordflow">if</span>(eigenValues[i] &gt; largestEigen) {
<a name="l00229"></a>00229                         largestEigen = eigenValues[i];
<a name="l00230"></a>00230                 }
<a name="l00231"></a>00231         }
<a name="l00232"></a>00232 
<a name="l00233"></a>00233         <span class="comment">// Return true (i.e. the genome is valid) if the largest eigen value is less than 0.</span>
<a name="l00234"></a>00234         <span class="keywordflow">if</span>(largestEigen &lt; 0.0) {
<a name="l00235"></a>00235                 <span class="keywordflow">return</span> <span class="keyword">true</span>;
<a name="l00236"></a>00236         }
<a name="l00237"></a>00237         <span class="keywordflow">else</span> {
<a name="l00238"></a>00238                 <span class="keywordflow">return</span> <span class="keyword">false</span>;
<a name="l00239"></a>00239         }
<a name="l00240"></a>00240 }
<a name="l00241"></a>00241 
<a name="l00242"></a><a class="code" href="class_tan_c_t_r_n_network.html#a52ce3fcdfc4999a5283b2efc20968d29">00242</a> <span class="keywordtype">int</span> <a class="code" href="class_tan_c_t_r_n_network.html#a52ce3fcdfc4999a5283b2efc20968d29">TanCTRNNetwork::getNoGenes</a>() {
<a name="l00243"></a>00243         <span class="keywordtype">int</span> genesForStrictScalingType = 0;
<a name="l00244"></a>00244         <span class="keywordflow">if</span>(myUseStrictScalingCondition) { genesForStrictScalingType += myNoNeurons; }
<a name="l00245"></a>00245         
<a name="l00246"></a>00246         <span class="keywordtype">int</span> noOutputGenes = 0;
<a name="l00247"></a>00247         <span class="keywordflow">if</span>(myUseOutputWeightsAndBiases) { noOutputGenes += 2 * <a class="code" href="class_neural_network.html#a18b0f9abda2d0127257162a605c3ac1a">myNoOutputs</a>;     }
<a name="l00248"></a>00248         
<a name="l00249"></a>00249         <span class="keywordflow">return</span> myNoNeurons * myNoNeurons <span class="comment">// Weights.</span>
<a name="l00250"></a>00250                         + myNoNeurons <span class="comment">// Biases.</span>
<a name="l00251"></a>00251                         + myNoNeurons <span class="comment">// Time Constants.</span>
<a name="l00252"></a>00252                         + genesForStrictScalingType <span class="comment">// strictScalingType (not always included).</span>
<a name="l00253"></a>00253                         + noOutputGenes <span class="comment">// If requested.</span>
<a name="l00254"></a>00254                         + 1; <span class="comment">// Sensor Weight.</span>
<a name="l00255"></a>00255 }
<a name="l00256"></a>00256 
<a name="l00257"></a><a class="code" href="class_tan_c_t_r_n_network.html#adadcb99b315db4ff9328f3a823c8141d">00257</a> <span class="keywordtype">void</span> <a class="code" href="class_tan_c_t_r_n_network.html#adadcb99b315db4ff9328f3a823c8141d">TanCTRNNetwork::resetNetwork</a>() {
<a name="l00258"></a>00258 
<a name="l00259"></a>00259         <span class="comment">// Reset all inputs to 0.</span>
<a name="l00260"></a>00260         <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; <a class="code" href="class_neural_network.html#a150a69555d44c2dd1015b51d843f474e">myNoInputs</a>; i++) {
<a name="l00261"></a>00261                 <a class="code" href="class_neural_network.html#a08225aaad97b0549739406d9347ebce4">myInputs</a>[i] = 0.0;
<a name="l00262"></a>00262         }
<a name="l00263"></a>00263 
<a name="l00264"></a>00264         <span class="comment">// Reset all states to 0.</span>
<a name="l00265"></a>00265         <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; myNoNeurons; i++) {
<a name="l00266"></a>00266                 myStates[i] = 0.0;
<a name="l00267"></a>00267         }
<a name="l00268"></a>00268 
<a name="l00269"></a>00269         <span class="comment">// Reset all outputs to 0.</span>
<a name="l00270"></a>00270         <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; <a class="code" href="class_neural_network.html#a18b0f9abda2d0127257162a605c3ac1a">myNoOutputs</a>; i++) {
<a name="l00271"></a>00271                 <a class="code" href="class_neural_network.html#ac1c0cc4ed2d147d5ae6bad210436cbd2">myOutputs</a>[i] = 0.0;
<a name="l00272"></a>00272         }
<a name="l00273"></a>00273 
<a name="l00274"></a>00274         myTimeSinceReset = 0;
<a name="l00275"></a>00275 }
<a name="l00276"></a>00276 
<a name="l00277"></a>00277 <span class="comment">// UPDATE -----------------------------------------------------------------------------------------------------</span>
<a name="l00278"></a><a class="code" href="class_tan_c_t_r_n_network.html#a0b847ff9aaa24af440b1a0c16b09030a">00278</a> <span class="keywordtype">void</span> <a class="code" href="class_tan_c_t_r_n_network.html#a0b847ff9aaa24af440b1a0c16b09030a">TanCTRNNetwork::updateNetwork</a>(<span class="keywordtype">double</span> timeStep) {
<a name="l00279"></a>00279         myTimeSinceReset += timeStep;
<a name="l00280"></a>00280 
<a name="l00281"></a>00281         <span class="comment">/* First record each neuron&#39;s post-tanh value, for efficiency.</span>
<a name="l00282"></a>00282 <span class="comment">         */</span>
<a name="l00283"></a>00283         <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; myNoNeurons; i++) {
<a name="l00284"></a>00284                 myPostTanhStateBuffer[i] = calcTanh(myStates[i] + myBiases[i]);
<a name="l00285"></a>00285         }
<a name="l00286"></a>00286 
<a name="l00287"></a>00287         <span class="keywordflow">for</span> (<span class="keywordtype">int</span> dest = 0; dest &lt; myNoNeurons; dest++) {
<a name="l00288"></a>00288                 <span class="comment">// Calculate the inputs.</span>
<a name="l00289"></a>00289                 <span class="keywordtype">double</span> input = 0.0;
<a name="l00290"></a>00290                 <span class="keywordflow">for</span> (<span class="keywordtype">int</span> src = 0; src &lt; myNoNeurons; src++) {
<a name="l00291"></a>00291                         input += myWeights[dest * myNoNeurons + src] * myPostTanhStateBuffer[src];
<a name="l00292"></a>00292                 }
<a name="l00293"></a>00293 
<a name="l00294"></a>00294                 <span class="comment">// Apply external inputs.</span>
<a name="l00295"></a>00295                 <span class="keywordflow">if</span> (dest &lt; <a class="code" href="class_neural_network.html#a150a69555d44c2dd1015b51d843f474e">myNoInputs</a>) {
<a name="l00296"></a>00296                         <span class="comment">// Apply inputs.</span>
<a name="l00297"></a>00297                         input += mySensorWeight * <a class="code" href="class_neural_network.html#a08225aaad97b0549739406d9347ebce4">myInputs</a>.at(dest);
<a name="l00298"></a>00298                 }
<a name="l00299"></a>00299 
<a name="l00300"></a>00300                 <span class="comment">// Update the neuron&#39;s state.</span>
<a name="l00301"></a>00301                 myNewStateBuffer[dest] = myStates[dest] + (timeStep
<a name="l00302"></a>00302                                 / myTimeConstants[dest]) * (-myStates[dest] + input);
<a name="l00303"></a>00303         }
<a name="l00304"></a>00304 
<a name="l00305"></a>00305         <span class="comment">// Update the neuron output records.</span>
<a name="l00306"></a>00306         myStates = myNewStateBuffer;
<a name="l00307"></a>00307 
<a name="l00308"></a>00308         <span class="comment">// Record the current neuron outputs.</span>
<a name="l00309"></a>00309         <span class="keywordflow">for</span> (<span class="keywordtype">int</span> o = 0; o &lt; <a class="code" href="class_neural_network.html#a18b0f9abda2d0127257162a605c3ac1a">myNoOutputs</a>; o++) {
<a name="l00310"></a>00310                 <span class="keywordflow">if</span>(myUseOutputWeightsAndBiases) {
<a name="l00311"></a>00311                         <a class="code" href="class_neural_network.html#ac1c0cc4ed2d147d5ae6bad210436cbd2">myOutputs</a>[o] = calcTanh(myOutputWeights.at(o) * myStates.at(myNoNeurons - myNoOutputs + o)
<a name="l00312"></a>00312                                                                         + myOutputBiases.at(o));
<a name="l00313"></a>00313                 }
<a name="l00314"></a>00314                         <span class="keywordflow">else</span> {
<a name="l00315"></a>00315                 <a class="code" href="class_neural_network.html#ac1c0cc4ed2d147d5ae6bad210436cbd2">myOutputs</a>[o] = calcTanh(myStates.at(myNoNeurons - myNoOutputs + o)
<a name="l00316"></a>00316                                 + myBiases.at(myNoNeurons - myNoOutputs + o));
<a name="l00317"></a>00317                         }
<a name="l00318"></a>00318                 
<a name="l00319"></a>00319                 <span class="keywordflow">if</span>(myScaleOutputToPositive) {
<a name="l00320"></a>00320                         <a class="code" href="class_neural_network.html#ac1c0cc4ed2d147d5ae6bad210436cbd2">myOutputs</a>[o] = (<a class="code" href="class_neural_network.html#ac1c0cc4ed2d147d5ae6bad210436cbd2">myOutputs</a>[o] + 1.0) * 0.5;
<a name="l00321"></a>00321                 }
<a name="l00322"></a>00322         }
<a name="l00323"></a>00323 }
<a name="l00324"></a>00324 
<a name="l00325"></a>00325 <span class="comment">// Scale weight by the largest eigenvalue. Must have initialized time constants too!</span>
<a name="l00326"></a>00326 <span class="keywordtype">void</span> TanCTRNNetwork::scaleWeights(<a class="code" href="class_genotype.html">Genotype</a>&amp; genotype) {
<a name="l00327"></a>00327         <span class="keywordflow">return</span>;
<a name="l00328"></a>00328         <span class="comment">// Use the strict (i.e. sufficient) scaling type --------------------------------------------------------------</span>
<a name="l00329"></a>00329         <span class="keywordflow">if</span>(myUseStrictScalingCondition) {
<a name="l00330"></a>00330                 <span class="comment">// Scale each row (i.e. each set of outgoing weights) such that Wii + 0.5 * sumJ(j != i [ |Wij| + |Wji|] ) &lt; -1   for all i.</span>
<a name="l00331"></a>00331                 <span class="keywordflow">for</span> (<span class="keywordtype">int</span> src = 0; src &lt; myNoNeurons; src++) {
<a name="l00332"></a>00332                         
<a name="l00333"></a>00333                         <span class="keywordtype">double</span> selfWeight = myWeights[src * myNoNeurons + src];
<a name="l00334"></a>00334                         <span class="keywordtype">double</span> sum = 0.0;
<a name="l00335"></a>00335                         <span class="keywordflow">for</span> (<span class="keywordtype">int</span> dest = 0; dest &lt; myNoNeurons; dest++) {
<a name="l00336"></a>00336                                 <span class="keywordflow">if</span>(src != dest) {
<a name="l00337"></a>00337                                         sum += abs(myWeights[dest * myNoNeurons + src]) + abs(myWeights[src * myNoNeurons + dest]);
<a name="l00338"></a>00338                                 }
<a name="l00339"></a>00339                         }
<a name="l00340"></a>00340                         
<a name="l00341"></a>00341                         <span class="keywordflow">if</span>((selfWeight + 0.5*sum) &lt; -1) {
<a name="l00342"></a>00342                                 <span class="comment">// Scale the appropriate side of the equation.</span>
<a name="l00343"></a>00343                                 <span class="keywordflow">if</span>(myStrictScalingType[src]) {
<a name="l00344"></a>00344                                         <span class="comment">// Scale the self weight.</span>
<a name="l00345"></a>00345                                         exit(1);
<a name="l00346"></a>00346                                 }
<a name="l00347"></a>00347                                 <span class="keywordflow">else</span> {
<a name="l00348"></a>00348                                         <span class="comment">// Scale the other weights.</span>
<a name="l00349"></a>00349                                         exit(1);
<a name="l00350"></a>00350                                 }
<a name="l00351"></a>00351                         }
<a name="l00352"></a>00352                         <span class="comment">// WRITE BACK INTO GENOME HERE</span>
<a name="l00353"></a>00353                         <span class="comment">//myWeights[dest * myNoNeurons + src]</span>
<a name="l00354"></a>00354                 }
<a name="l00355"></a>00355         }
<a name="l00356"></a>00356         
<a name="l00357"></a>00357         <span class="comment">// Use the necessary (i.e. not necessarily sufficient) scaling type -----------------------------------------</span>
<a name="l00358"></a>00358         <span class="keywordflow">else</span> {
<a name="l00359"></a>00359                         
<a name="l00360"></a>00360                 <span class="comment">// Put the weights in the format required by the JAMA eigen decomposition class.</span>
<a name="l00361"></a>00361                 TNT::Array2D&lt;double&gt; weightMatrixTNT(myNoNeurons, myNoNeurons);
<a name="l00362"></a>00362                 <span class="keywordflow">for</span> (<span class="keywordtype">int</span> dest = 0; dest &lt; myNoNeurons; dest++) {
<a name="l00363"></a>00363                         <span class="keywordflow">for</span> (<span class="keywordtype">int</span> source = 0; source &lt; myNoNeurons; source++) {
<a name="l00364"></a>00364                                 <span class="comment">// XXX RIGHT WAY ROUND ??? Don&#39;t think it matters!</span>
<a name="l00365"></a>00365                                 weightMatrixTNT[source][dest] = myWeights[dest * myNoNeurons + source];
<a name="l00366"></a>00366                         }       
<a name="l00367"></a>00367                 }
<a name="l00368"></a>00368         
<a name="l00369"></a>00369                 <span class="comment">// Get the largest eigenvalues.</span>
<a name="l00370"></a>00370                 JAMA::Eigenvalue&lt;double&gt; eigen(weightMatrixTNT);
<a name="l00371"></a>00371                 TNT::Array1D&lt;double&gt; eigenValues;
<a name="l00372"></a>00372                 eigen.getRealEigenvalues(eigenValues);
<a name="l00373"></a>00373                 <span class="keywordtype">double</span> largestEigen = -99999999;
<a name="l00374"></a>00374                 <span class="keywordflow">for</span>(<span class="keywordtype">int</span> i = 0; i &lt; myNoNeurons; i++) {
<a name="l00375"></a>00375                         <span class="keywordflow">if</span>(eigenValues[i] &gt; largestEigen) {
<a name="l00376"></a>00376                                 largestEigen = eigenValues[i];
<a name="l00377"></a>00377                         }
<a name="l00378"></a>00378                 }
<a name="l00379"></a>00379                 
<a name="l00380"></a>00380                 <span class="comment">// Scale the weights by the largest eigenvector (plus a little), only if that eigenvalue was greater than 1.</span>
<a name="l00381"></a>00381                 <span class="keywordflow">if</span>(largestEigen &gt;= 1.0) {
<a name="l00382"></a>00382                         <span class="keywordflow">for</span> (<span class="keywordtype">int</span> dest = 0; dest &lt; myNoNeurons; dest++) {
<a name="l00383"></a>00383                                 <span class="keywordflow">for</span> (<span class="keywordtype">int</span> source = 0; source &lt; myNoNeurons; source++) {
<a name="l00384"></a>00384                                         myWeights[dest * myNoNeurons + source] = myWeights[dest * myNoNeurons + source] / (largestEigen + myScaleModifier); <span class="comment">// XXX RIGHT WAY ROUND ???</span>
<a name="l00385"></a>00385                                 }
<a name="l00386"></a>00386                         }
<a name="l00387"></a>00387                 }
<a name="l00388"></a>00388                 
<a name="l00389"></a>00389                 <span class="comment">// Write these weights back into the genotype.</span>
<a name="l00390"></a>00390                 writeWeightsToGenotype(genotype);
<a name="l00391"></a>00391         }
<a name="l00392"></a>00392 }
<a name="l00393"></a>00393 
<a name="l00394"></a>00394 <span class="keywordtype">void</span> TanCTRNNetwork::writeWeightsToGenotype(<a class="code" href="class_genotype.html">Genotype</a>&amp; genotype) {
<a name="l00395"></a>00395         <span class="comment">// Write these weights back into the genotype.</span>
<a name="l00396"></a>00396         <span class="keywordtype">int</span> geneNo = 0;
<a name="l00397"></a>00397         <span class="keywordflow">for</span> (<span class="keywordtype">int</span> dest = 0; dest &lt; myNoNeurons; dest++) {
<a name="l00398"></a>00398                 <span class="keywordflow">for</span> (<span class="keywordtype">int</span> source = 0; source &lt; myNoNeurons; source++) {
<a name="l00399"></a>00399                         <span class="comment">// Note inputs now have incoming weights. Is this good?</span>
<a name="l00400"></a>00400                         <span class="keywordflow">if</span>(myConstrainWithScaledWeights &amp;&amp; (dest == source)) {
<a name="l00401"></a>00401                                 <span class="comment">// Self weight is negative.</span>
<a name="l00402"></a>00402                                 genotype.<a class="code" href="class_genotype.html#a86c48c2c8760026f02991b92065d6242">myGenes</a>.at(geneNo) = myWeights[dest * myNoNeurons + source] / -myMaxWeight;
<a name="l00403"></a>00403                         }
<a name="l00404"></a>00404                                 <span class="keywordflow">else</span> {
<a name="l00405"></a>00405                                         genotype.<a class="code" href="class_genotype.html#a86c48c2c8760026f02991b92065d6242">myGenes</a>.at(geneNo) = (myWeights[dest * myNoNeurons + source] + myMaxWeight) / (2*myMaxWeight);
<a name="l00406"></a>00406                                 }
<a name="l00407"></a>00407                         geneNo++;
<a name="l00408"></a>00408                 }
<a name="l00409"></a>00409         }
<a name="l00410"></a>00410 }
<a name="l00411"></a>00411 
<a name="l00412"></a><a class="code" href="class_tan_c_t_r_n_network.html#a68365d4d82c00d1c0d7e0936f9c2edbf">00412</a> <span class="keywordtype">void</span> <a class="code" href="class_tan_c_t_r_n_network.html#a68365d4d82c00d1c0d7e0936f9c2edbf">TanCTRNNetwork::resetRecords</a>() {
<a name="l00413"></a>00413 
<a name="l00414"></a>00414         <span class="comment">// Clear the state containers.</span>
<a name="l00415"></a>00415         myStateRecord.clear();
<a name="l00416"></a>00416         myInputRecord.clear();
<a name="l00417"></a>00417         myOutputRecord.clear();
<a name="l00418"></a>00418 }
<a name="l00419"></a>00419 
<a name="l00420"></a><a class="code" href="class_tan_c_t_r_n_network.html#a0e9d559f621259b247219d3655f5af81">00420</a> <span class="keywordtype">void</span> <a class="code" href="class_tan_c_t_r_n_network.html#a0e9d559f621259b247219d3655f5af81">TanCTRNNetwork::updateRecords</a>() {
<a name="l00421"></a>00421         myStateRecord.push_back(myStates);
<a name="l00422"></a>00422         myInputRecord.push_back(<a class="code" href="class_neural_network.html#a08225aaad97b0549739406d9347ebce4">myInputs</a>);
<a name="l00423"></a>00423         myOutputRecord.push_back(<a class="code" href="class_neural_network.html#ac1c0cc4ed2d147d5ae6bad210436cbd2">myOutputs</a>);
<a name="l00424"></a>00424 }
<a name="l00425"></a>00425 
<a name="l00426"></a><a class="code" href="class_tan_c_t_r_n_network.html#adf89c4ec227f9723e629e22eaedb44de">00426</a> <span class="keywordtype">string</span> <a class="code" href="class_tan_c_t_r_n_network.html#adf89c4ec227f9723e629e22eaedb44de">TanCTRNNetwork::getRecords</a>() {
<a name="l00427"></a>00427         <span class="comment">// Output the network configuration and the state since recording started.</span>
<a name="l00428"></a>00428         stringstream record;
<a name="l00429"></a>00429 
<a name="l00430"></a>00430         record &lt;&lt; <a class="code" href="class_matlab_writer.html#acc0a93953322177b2845bb4b90cee17d">MatlabWriter::parse</a>(myBiases, <span class="stringliteral">&quot;bias&quot;</span>);
<a name="l00431"></a>00431         record &lt;&lt; MatlabWriter::parse(myTimeConstants, <span class="stringliteral">&quot;tau&quot;</span>);
<a name="l00432"></a>00432         record &lt;&lt; MatlabWriter::parse(myScaleModifier, <span class="stringliteral">&quot;scaleModifier&quot;</span>);
<a name="l00433"></a>00433         record &lt;&lt; MatlabWriter::parse(myUseStrictScalingCondition, <span class="stringliteral">&quot;strictScalingCondition&quot;</span>);
<a name="l00434"></a>00434         
<a name="l00435"></a>00435         <span class="keywordflow">if</span>(myUseStrictScalingCondition) {
<a name="l00436"></a>00436                 record &lt;&lt; MatlabWriter::parse(myStrictScalingType, <span class="stringliteral">&quot;strictScalingType&quot;</span>);
<a name="l00437"></a>00437         }
<a name="l00438"></a>00438         
<a name="l00439"></a>00439         record &lt;&lt; MatlabWriter::parse(myUseOutputWeightsAndBiases, <span class="stringliteral">&quot;useOutputWeightsAndBiases&quot;</span>);
<a name="l00440"></a>00440         <span class="keywordflow">if</span>(myUseOutputWeightsAndBiases) {
<a name="l00441"></a>00441                 record &lt;&lt; MatlabWriter::parse(myOutputWeights, <span class="stringliteral">&quot;outputWeights&quot;</span>);
<a name="l00442"></a>00442                 record &lt;&lt; MatlabWriter::parse(myOutputBiases, <span class="stringliteral">&quot;outputBiases&quot;</span>);
<a name="l00443"></a>00443         }
<a name="l00444"></a>00444         
<a name="l00445"></a>00445         <span class="comment">// Read the weights into a 2D matrix for recording.</span>
<a name="l00446"></a>00446         vector&lt;vector&lt;double&gt; &gt; weights2D(myNoNeurons, vector&lt;double&gt;(myNoNeurons));
<a name="l00447"></a>00447         <span class="keywordflow">for</span>(<span class="keywordtype">int</span> src = 0; src &lt; myNoNeurons; src++) {
<a name="l00448"></a>00448                 <span class="keywordflow">for</span>(<span class="keywordtype">int</span> dest = 0; dest &lt; myNoNeurons; dest++) {
<a name="l00449"></a>00449                         weights2D.at(src).at(dest) = myWeights.at(dest * myNoNeurons + src);
<a name="l00450"></a>00450                 }       
<a name="l00451"></a>00451         }
<a name="l00452"></a>00452         record &lt;&lt; MatlabWriter::parse(weights2D, <span class="stringliteral">&quot;weights&quot;</span>);
<a name="l00453"></a>00453         record &lt;&lt; MatlabWriter::parse(mySensorWeight, <span class="stringliteral">&quot;sensorWeight&quot;</span>);
<a name="l00454"></a>00454         record &lt;&lt; MatlabWriter::parse(myUseLookup, <span class="stringliteral">&quot;useLookup&quot;</span>);
<a name="l00455"></a>00455         record &lt;&lt; MatlabWriter::parse(myStateRecord, <span class="stringliteral">&quot;neuronStates&quot;</span>);
<a name="l00456"></a>00456         record &lt;&lt; MatlabWriter::parse(myInputRecord, <span class="stringliteral">&quot;neuronInputs&quot;</span>);
<a name="l00457"></a>00457         record &lt;&lt; MatlabWriter::parse(myOutputRecord, <span class="stringliteral">&quot;neuronOutputs&quot;</span>);
<a name="l00458"></a>00458         
<a name="l00459"></a>00459         record &lt;&lt; MatlabWriter::parse(myMaxWeight, <span class="stringliteral">&quot;maxWeight&quot;</span>);
<a name="l00460"></a>00460         record &lt;&lt; MatlabWriter::parse(myInitialWeightModifier, <span class="stringliteral">&quot;initialWeightModifier&quot;</span>);
<a name="l00461"></a>00461         record &lt;&lt; MatlabWriter::parse(myInitialWeightsNormal, <span class="stringliteral">&quot;initalWeightsNormal&quot;</span>);
<a name="l00462"></a>00462 
<a name="l00463"></a>00463         <span class="keywordflow">return</span> record.str();
<a name="l00464"></a>00464 }
<a name="l00465"></a>00465 
<a name="l00466"></a><a class="code" href="class_tan_c_t_r_n_network.html#a3fe7dd0cc108d5240362aab031aec8ad">00466</a> <span class="keywordtype">void</span> <a class="code" href="class_tan_c_t_r_n_network.html#a3fe7dd0cc108d5240362aab031aec8ad">TanCTRNNetwork::addProperties</a>(<a class="code" href="class_properties.html">Properties</a>&amp; properties) {
<a name="l00467"></a>00467         properties.<a class="code" href="class_properties.html#a2c21846f1e3256cd344efcba7587c830">addIntItem</a>(<span class="stringliteral">&quot;noInterneurons&quot;</span>);
<a name="l00468"></a>00468         properties.<a class="code" href="class_properties.html#a52de4f3cc9dc0590f71547ad1e95ba64">addBoolItem</a>(<span class="stringliteral">&quot;seedWithCentreCrossing&quot;</span>);
<a name="l00469"></a>00469         properties.<a class="code" href="class_properties.html#a52de4f3cc9dc0590f71547ad1e95ba64">addBoolItem</a>(<span class="stringliteral">&quot;seedWithScaledWeights&quot;</span>);
<a name="l00470"></a>00470         properties.<a class="code" href="class_properties.html#a52de4f3cc9dc0590f71547ad1e95ba64">addBoolItem</a>(<span class="stringliteral">&quot;constrainWithScaledWeights&quot;</span>);
<a name="l00471"></a>00471         properties.<a class="code" href="class_properties.html#a52de4f3cc9dc0590f71547ad1e95ba64">addBoolItem</a>(<span class="stringliteral">&quot;useLookup&quot;</span>);
<a name="l00472"></a>00472         properties.<a class="code" href="class_properties.html#a52de4f3cc9dc0590f71547ad1e95ba64">addBoolItem</a>(<span class="stringliteral">&quot;scaleOutputToPositive&quot;</span>);
<a name="l00473"></a>00473         properties.<a class="code" href="class_properties.html#a52de4f3cc9dc0590f71547ad1e95ba64">addBoolItem</a>(<span class="stringliteral">&quot;setTanhTausToOne&quot;</span>);
<a name="l00474"></a>00474         properties.<a class="code" href="class_properties.html#ae42d6e72ac230667da61f13e34f6710b">addDoubleItem</a>(<span class="stringliteral">&quot;scaleModifier&quot;</span>);
<a name="l00475"></a>00475         properties.<a class="code" href="class_properties.html#a52de4f3cc9dc0590f71547ad1e95ba64">addBoolItem</a>(<span class="stringliteral">&quot;strictScalingCondition&quot;</span>);
<a name="l00476"></a>00476         properties.<a class="code" href="class_properties.html#a52de4f3cc9dc0590f71547ad1e95ba64">addBoolItem</a>(<span class="stringliteral">&quot;setBiasesToZero&quot;</span>);
<a name="l00477"></a>00477         properties.<a class="code" href="class_properties.html#a52de4f3cc9dc0590f71547ad1e95ba64">addBoolItem</a>(<span class="stringliteral">&quot;useOutputWeightsAndBiases&quot;</span>);
<a name="l00478"></a>00478         properties.<a class="code" href="class_properties.html#ae42d6e72ac230667da61f13e34f6710b">addDoubleItem</a>(<span class="stringliteral">&quot;maxWeight&quot;</span>);
<a name="l00479"></a>00479         properties.<a class="code" href="class_properties.html#ae42d6e72ac230667da61f13e34f6710b">addDoubleItem</a>(<span class="stringliteral">&quot;initialWeightModifier&quot;</span>);
<a name="l00480"></a>00480         properties.<a class="code" href="class_properties.html#ae42d6e72ac230667da61f13e34f6710b">addDoubleItem</a>(<span class="stringliteral">&quot;initalWeightsNormal&quot;</span>);
<a name="l00481"></a>00481 }
</pre></div></div>
</div>
  <div id="nav-path" class="navpath">
    <ul>
      <li class="navelem"><a class="el" href="_old_tan_c_t_r_n_network_8cpp.html">OldTanCTRNNetwork.cpp</a>      </li>
      <li class="footer">Generated on Wed May 25 2011 21:39:04 for Robust Sensors by&#160;
<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.7.4 </li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&#160;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&#160;</span>Classes</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&#160;</span>Files</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&#160;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(4)"><span class="SelectionMark">&#160;</span>Variables</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(5)"><span class="SelectionMark">&#160;</span>Typedefs</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(6)"><span class="SelectionMark">&#160;</span>Enumerator</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(7)"><span class="SelectionMark">&#160;</span>Friends</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(8)"><span class="SelectionMark">&#160;</span>Defines</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>


</body>
</html>
